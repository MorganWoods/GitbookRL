---
description: "Tasks and ideas \U0001F4A1"
---

# Tasks & ideas🔅

## Tasks 🌀

### ➡ Upcoming tasks

* 看文章,准备下一篇.
* ~~把 Reinforcement Learning 相关笔记与知识迁移到 Gitbook 上 迁过去吧,记笔记看笔记方便一些.但这里保存原来的几个文件,且暂时停止更新.~~

  ~~已经迁移到 gitbook 上的 RL 文档中:~~[~~https://kiitos.gitbook.io/rl/~~](https://kiitos.gitbook.io/rl/) 目前纸质笔记迁移中, ~~接下来把工具操作相关文档也迁移到 gitbook 的 tool 中.~~

* ~~写实验部分时候参考别的论文的实验用语. 加油吧,提高效率.~~
* ~~完成论文\(20-May\), 修改论文\(30-May\); 加 on policy 版本算法\(30-May\); 五月末完成论文, 6月准备投稿.~~
* ~~查找会议与期刊.~~
* OneNote 笔记迁移到 Github; 论文公式与绘图迁移到 RLKnowledge 文件中.个人笔记也弄到 github 上来. 无纸化.
* 下次读文章时候把参考文献记录下来,分类.

### ➡ Potential tasks

* enhance conference talking ability.

## Planning 🌀

### ➡ Journal\_2018

Oct.5 - Oct.15 finish rewriting  
Oct.15-Oct.30. finish submit.

### ➡ Conference\_2019.4 

## All ideals 🌀

> 目前关注 reward 机制. Lstm 机制;  
> 为了找工作:需要关注 大数据, 图像视频处理, deep learning. 推荐算法, 这一类. 单独的增强学习可能优势不足.  
> 下面按照类别划分出来.

* Gridworld 改成机器人类似的形式,把终点信息加入 reward 中,加强其泛化能力;
* 类似 NAF 那样把两个网络完成的事弄成一个网络完成,看可行不可行;
* gazebo 环境的开发
* [https://www.youtube.com/watch?v=EaY5QiZwSP4](https://www.youtube.com/watch?v=EaY5QiZwSP4) 一个无人车模拟,可以尝试
* 研究 LSTM 算法
* 关于奖励稀疏的问题, 最终奖励过于稀疏不利于训练. 参考这个网页的解释[http://swarma.blog.caixin.com/archives/164137](http://swarma.blog.caixin.com/archives/164137)也可以归类于 reward shaping 类别下.
* VIN 应用到连续空间可行么?
* 使用增强网络建造神经网络结构.
* using imageNet to finish some work? 关于图像方面是一个重点,还有卷积神经网络.
* GAN and RL cooperation?
* Transfer learning&gt;? GAN should be useful for it.
* 关于贪婪算法,也有提升空间, oleksii 论文说的 Upper confidence bound 算法.
* 在同一个任务中用不同的网络干不同的事,分开干. 一个主网络负责调用各个分网络,各个分网络自己干自己的事. 这也是分级网络的逻辑.
* Dual attention technique.

### ➡ experience replay

* 对 reward 的机制下手, 让他得分惩罚更加合理有利于收敛. reward shaping
* PER 那个, 以后注意一下, 经历分出优先级使用.




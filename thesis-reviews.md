---
description: aalto's thesis reviews
---

# Thesis reviewsüéì

## Graph signal sampling via reinforcement learning

> Master's thesis, Oleksii Abramenko , 2018 , pages 58,
>
> cardinality:Âäø ;  disposal Â§ÑÁêÜ,ÁÆ°ÁêÜ; applicability ÈÄÇÁî®ÊÄß;

1. Graph signal processing area and it's practical applications such as compression and image denoising. This thesis applys RL theory to aforementioned problem. He propose two RL based sampling algorithms.
2. Abbreviations: GSP: graph signal processing   SLP:sparse label propagation    LASSO: least absolute shrinkage and selection operator    TV: total variation    SBM: stochastic block model     RWS: random walk sampling     URS: uniform random sampling
3. **contents**:  C1: Intro  C2: ML for GSP    C3: RL fundamentals   C4: Graph signal sampling as RL problem   C5: experiments   C6:discussion   C7:conclusions
4. The nodes of graph: network elements; edges: the connections of elements; 1st challenge: representative samples selection; 2nd challenge: how to recover the required info. _**Goal**: Learn a sampling policy choosing signal samples with small reconstruction error_.
5. Upper confidence bound approach ÂèØ‰ª•Êõ¥Â•ΩÁöÑÊõø‰ª£ Ë¥™Â©™ÁÆóÊ≥ï; ‰ªñËÉΩË°°ÈáèÂáÜÁ°ÆÂ∫¶.
6. ÁúãÂà∞‰∫Ü4.2Á´†. ÂÖàÁúãÂà´ÁöÑ. we consider the selection of the nodes to be sampled being carried out by an ‚Äúagent‚Äù which crawls over the data graph G A specific action a 2 A refers to the number of hops the sampling agent performs starting at the current node it to reach a new node it+1, which will be added to the sampling set, i.e., M := M \[ {it+1}

> correspond to; whereas; reasoning, Noun, The action of thinking about something in a logical, sensible way.; come down to, ÂΩíÁªì‰∏∫; preclude, prevent from happening, make impossible; derivative Ë°çÁîü; consequently, as a result; be identical to , similar as     ;     represent ‰ª£Ë°®,Ë°®Á§∫,ÂëàÁé∞;       proportional ÊàêÊØî‰æãÁöÑ; consensus, general agreement ÂÖ±ËØÜ       ;  essence, intrinsic nature or indispensable quality of sth,Êú¨Ë¥®        ; responsible for ...     ; originate from Ê∫ê‰∫é;  consecutive, following each other continuously,ËøûÁª≠ÁöÑ;  underlying Áõ∏ÂÖ≥;   denote , be a sign of, indicate;   linearly decrease Á∫øÊÄß;   proceed, go on, begin a course of action ÁªßÁª≠ËøõÁ®ã;   course, ËØæÁ®ã,ËøõÁ®ã,Ë∑ØÁ∫ø.;    amount to , ÂÖ±ËÆ°, ÊÑèÂë≥ÁùÄ, ÂèëÂ±ïÊàê;  retain, continue to have sth, keep possession of.;    synchronise, sync;    tend to Ë∂ã‰∫é;  reward Êä•ÈÖ¨,Á≠îË∞¢;   basic adj,  basics n;   vestly, adv Â§ßÂ§ßÂú∞;     hop Ë∑≥Ë∑É;     prescribe ËßÑÂÆö,ËçØÊñπ;      assess ËØÑ‰º∞;      convex Âá∏ÁöÑ.;    compact ÂêàÂêå,Â•ëÁ∫¶;      provoke ÊøÄÂä±,ÁÖΩÂä®;       detrimental, harmful. ;       ambiguous Ê®°Ê£±‰∏§ÂèØÁöÑ;     paradigmËåÉ‰æã   ;       derivation ÂºïÂá∫,ËØ±ÂØº,Êù•ÂéÜ;      problematic ÈóÆÈ¢òÁöÑ,ÊúâÁñëÈóÆÁöÑ;       disputable  Êúâ‰∫âËÆÆÁöÑ,ÂèØÁñëÁöÑ;       interval   Èó¥Èöî, ‰ºëÊÅØ;       underlying Áõ∏ÂÖ≥ÁöÑ;       accordance   Á¨¶Âêà,ÂêåÊÑè;       consistent  ‰∏ÄË¥Ø;        prospect ÂâçÈÄî,ÊôØËâ≤;         in comparison to;         tendency Ë∂ãÂêë,Ë∂ãÂäø;           intractability   ÈöæËß£;         stateless Êó†Áä∂ÊÄÅ;           diameter Áõ¥ÂæÑ;       cliques  Ê¥æÁ≥ª,Â∞èÈõÜÂõ¢;      depict  ÊèèËø∞;



## A gentle Introduction of RL

> A Fixed Point View on Cleaning Robots - Alexander Jung  
> Reviewed by Menghao Wu

#### 1 Introduction

* Actually,I suppose the relationship between the cleaning robot and the grid world is weak. Since robot's observation is snapshots or distance information , however the agent's observation in the grid world is a birds-eye view \(agent know everything e.g. obstacles, terminal ‚Ä¶ in grid world\). Logically,it seems that we should use a camera on the rooftop to see the cleaning robot and the floor and these should be the observation of robot. From my point of view, grid world is good enough to describe the RL's process

#### 2 MDP

* 2.1 The Elements of MDP
  * The hypothesis of Rumba can determine the position is wise .
  * To my way of thinking, Markov property's Introduction should be put at the beginning of **MDP** paragraph, and the intrduction should be more detailed because it is a basic ideal of all RL setting . It cuts off the relationship between the past and future states, but using discounted  reward to connect the whole process. Then  is a essential factor of MDP tuple.
  * "The discounted factor  trades-off the importance of immediate and future rewards" ‚Üê this sentence I found in paper seems more accurate to introduce  .
* 2.2 Policies
  * Up to **now** \(know\) ? 1st sentence of 2.2
* 2.5 Policy Improvement
  * Policy improvement via chooseing the action maximums the Q value, this is a value-based way, with the corresponding Policy-based methods are often used for Continuous control problem. Therefore this section is a thought to handle the discrete actions' tasks, the conditions of the scene can be introduced in detail.

#### 3 Computing Optimal Policies for Known MDPs

* 3.3 Return Shaping
  * Thsi section is extremely good since the metaphor is very appropriate. Return shaping is potential technique to improve perfoemance of training.

#### 4 Machine Learning as Low-Level AI

* I have used the thought as you write to process snapshots as labels relate to the location information. Eventhough the results are overfitting , it is a good idea to map location from the snapshot. However There is a big problem in real world because the robot moving Continuously so it often shots scene not in the labeled data. This will cause robot confuseing and labeled data not big enough too. robot can take snapshots in different angles , directions, locations . So it will come across many hard problem to handle in the real environment.


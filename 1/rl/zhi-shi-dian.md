---
description: RL Basic theory
---

# Basics

## Basic terms

* **Three functions**
  * **Value function V**: measure how good it is to be in a particular states.; total reward from s
  * **Q function**: measure the value of choosing a particular action when in this state. Q: action-value; total reward from taking a in s
  * **Advantage function**: subtract the value of the state from the Q function to obtain a relative measure of the importance of each action.;how much better a is.
* DQN 's important factors are **Target network** and **Experience replay**.
* During the learning period, randomly select samples from the experience to build the Loss function.
* $$\theta$$ is weights and bias \(parameters\) of neural networks.
* The **hyperparameters** means the pre-set values like learning rate , etc. cannot learn from training period.
* **DL** is a field of representation learning; **ML** is the necessary condition of DL; DL is a special form of ML; ML is learning algorithm from raw data; Learning: From experience E, task T, performance measure;
* Some typical ML's tasks: Classification, Regression, Transcription\(recognize character from image\), Machine translation, Structured output, Anomaly detection\(å¼‚å¸¸æ£€æµ‹\), Imputation of missing value, Denoising, Density estimation or probability mass function estimation.
* When face a high dimensional data, ML may not work since the **curse of dimensionality**.  Some DL's models: Deep feedforward network, MLPs, feedforward neural networks.
* The goal of MLP is learning Î¸ from training data. It has hidden layers and output layer. Actually it is similar like a set of f chains. The length of f chains called **depth**, the number of units called **width**.
* To the output layer, choosing a proper **cost function** is a necessity condition. The main object of DL is minimizing the Loss function. For avoiding **overfitting**, adding a **weight decay** term in **regularization** of cost function. It's a coefficient in front of regularization term.
* **AI** is a enormous and general concept, ML is a method to realize the AI world. A big problem of AI is **Feature extraction**, DL can solve is well, DL is a technique of ML.
* **Model free**: do not know anything of env before, waiting for feedback to implement next action. **Model based**: understand env, can predict the future results of actions.
* **Policy-based**: choosing actions according to policy. e.g. PG, DDPG. **Value-based**: Choosing the action with the highest value. e.g. Q-learning, sarsa.

## Actor-Critc 

è¿™æ˜¯ on policy çš„. è¿è¡Œæµç¨‹: A æ ¹æ® state é€‰å‡ºä¸€ä¸ª action â¡ï¸ C æ ¹æ® state å’Œ action å¯¹ A çš„è¡¨ç°æ‰“åˆ† â¡ï¸ Aæ ¹æ® C çš„æ‰“åˆ†,è°ƒæ•´è‡ªå·±çš„ç­–ç•¥\(ç½‘ç»œå‚æ•°\) â¡ï¸ C æ ¹æ®ç³»ç»Ÿçš„ reward å’Œ C çš„ target è°ƒæ•´è‡ªå·±æ‰“åˆ†ç­–ç•¥\(ç½‘ç»œå‚æ•°\)ä¸€å¼€å§‹ A ä¸ C éƒ½æ˜¯éšæœºçš„,ä½†æ˜¯ç”±äº reward çš„å­˜åœ¨,äºŒè€…ç­–ç•¥è¶Šæ¥è¶Šå‡†; A è°ƒæ•´å‚æ•°æ ¹æ® C çš„æ‰“åˆ†,è¿åˆ C.

## Policy

* Policy: maps state to action, optimization is to find an optimal mapping.
* episodic tasks - æƒ…èŠ‚æ€§ä»»åŠ¡ã€‚æŒ‡ï¼ˆå¼ºåŒ–å­¦ä¹ çš„é—®é¢˜ï¼‰ä¼šåœ¨æœ‰é™æ­¥éª¤ä¸‹ç»“æŸã€‚continuing tasks - è¿ç»­æ€§ä»»åŠ¡ã€‚æŒ‡ï¼ˆå¼ºåŒ–å­¦ä¹ çš„é—®é¢˜ï¼‰æœ‰æ— é™æ­¥éª¤ã€‚episode - æƒ…èŠ‚ã€‚æŒ‡ä»èµ·å§‹çŠ¶æ€ï¼ˆæˆ–è€…å½“å‰çŠ¶æ€ï¼‰åˆ°ç»“æŸçš„æ‰€æœ‰æ­¥éª¤ã€‚tabular method - åˆ—è¡¨æ–¹æ³•ã€‚æŒ‡ä½¿ç”¨äº†æ•°ç»„æˆ–è€…è¡¨æ ¼å­˜å‚¨æ¯ä¸ªçŠ¶æ€ï¼ˆæˆ–è€…çŠ¶æ€-è¡ŒåŠ¨ï¼‰çš„ä¿¡æ¯ï¼ˆæ¯”å¦‚ï¼šå…¶ä»·å€¼ï¼‰ã€‚
* planning method - è®¡åˆ’æ€§æ–¹æ³•ã€‚éœ€è¦ä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨æ¨¡å‹é‡Œï¼Œå¯ä»¥è·å¾—çŠ¶æ€ä»·å€¼ã€‚æ¯”å¦‚ï¼š åŠ¨æ€è§„åˆ’ã€‚learning method - å­¦ä¹ æ€§æ–¹æ³•ã€‚ä¸éœ€è¦æ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿï¼ˆæˆ–è€…ä½“éªŒï¼‰ï¼Œæ¥è®¡ç®—çŠ¶æ€ä»·å€¼ã€‚æ¯”å¦‚ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•ï¼Œæ—¶åºå·®åˆ†æ–¹æ³•ã€‚
* on-policy method - on-policyæ–¹æ³•ã€‚è¯„ä¼°çš„ç­–ç•¥å’Œä¼˜åŒ–çš„ç­–ç•¥æ˜¯åŒä¸€ä¸ªã€‚
* off-policy method - off-policyæ–¹æ³•ã€‚è¯„ä¼°çš„ç­–ç•¥å’Œä¼˜åŒ–çš„ç­–ç•¥ä¸æ˜¯åŒä¸€ä¸ªã€‚æ„å‘³ç€ä¼˜åŒ–ç­–ç•¥ä½¿ç”¨æ¥è‡ªå¤–éƒ¨çš„æ ·æœ¬æ•°æ®ã€‚
* target policy - ç›®æ ‡ç­–ç•¥ã€‚off-policyæ–¹æ³•ä¸­éœ€è¦ä¼˜åŒ–çš„ç­–ç•¥ã€‚
* behavior policy - è¡Œä¸ºç­–ç•¥Î¼ã€‚off-policyæ–¹æ³•ä¸­æä¾›æ ·æœ¬æ•°æ®çš„ç­–ç•¥
* importance sampling - è¡Œä¸ºç­–ç•¥Î¼çš„æ ·æœ¬æ•°æ®ã€‚
* importance sampling rate - ç”±äºç›®æ ‡ç­–ç•¥Ï€å’Œè¡Œä¸ºç­–ç•¥Î¼ä¸åŒï¼Œå¯¼è‡´æ ·æœ¬æ•°æ®åœ¨ä½¿ç”¨ä¸Šçš„åŠ æƒå€¼ã€‚
* ordinary importance sampling - æ— åè§çš„è®¡ç®—ç­–ç•¥ä»·å€¼çš„æ–¹æ³•ã€‚
* weighted importance sampling - æœ‰åè§çš„è®¡ç®—ç­–ç•¥ä»·å€¼çš„æ–¹æ³•ã€‚
* MSE\(mean square error\) - å¹³å‡å¹³æ–¹è¯¯å·®ã€‚
* MDP\(markov decision process\) - é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹
* â‰ - å®šä¹‰ä¸Šçš„ç­‰ä»·å…³ç³»ã€‚E\[X\] - Xçš„æœŸæœ›å€¼ã€‚Pr{X=x} - å˜é‡Xå€¼ä¸ºxçš„æ¦‚ç‡ã€‚vâ†¦g - væ¸è¿‘gã€‚ğŸ‘‚vâ‰ˆg - vçº¦ç­‰äºgã€‚R - å®æ•°é›†åˆã€‚Rn - nä¸ªå…ƒç´ çš„å®æ•°å‘é‡ã€‚maxaâˆˆA F\(a\) - åœ¨æ‰€æœ‰çš„è¡ŒåŠ¨ä¸­ï¼Œæ±‚æœ€å¤§å€¼F\(a\)ã€‚argmaxc F\(c\) - æ±‚å½“F\(c\)ä¸ºæœ€å¤§å€¼æ—¶ï¼Œå‚æ•°cçš„å€¼ã€‚
* unbiased estimate :æ— åä¼°è®¡,ç”¨æ ·æœ¬ç»Ÿè®¡é‡æ¥ä¼°è®¡æ€»ä½“å‚æ•°æ—¶çš„ä¸€ç§æ— åæ¨æ–­.ä¼°è®¡é‡çš„æ•°å­¦æœŸæœ›ç­‰äºè¢«ä¼°è®¡å‚æ•°çš„çœŸå®å€¼,å…·æœ‰æ— åæ€§,æ˜¯ä¸€ç§ç”¨äºè¯„ä»·ä¼°è®¡é‡ä¼˜è‰¯æ€§çš„å‡†åˆ™.


---
description: Question collection
---

# Q&A

## Policy gradient ğŸŒ€

### â¡ explain the Loss function of PG

Loss = -log\(prob\)\*vt

log\(prob\) denotes the surprising degree of selecting action a on state s.   
ä¸Šå¼ä¸­log\(prob\)è¡¨ç¤ºåœ¨çŠ¶æ€ s å¯¹æ‰€é€‰åŠ¨ä½œ a çš„åƒæƒŠåº¦, å¦‚æœæ¦‚ç‡è¶Šå°, åå‘çš„log\(prob\) åè€Œè¶Šå¤§. è€Œvtä»£è¡¨çš„æ˜¯å½“å‰çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaæ‰€èƒ½å¾—åˆ°çš„å¥–åŠ±ï¼Œè¿™æ˜¯å½“å‰çš„å¥–åŠ±å’Œæœªæ¥å¥–åŠ±çš„è´´ç°å€¼çš„æ±‚å’Œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•å¿…é¡»è¦å®Œæˆä¸€ä¸ªå®Œæ•´çš„eposideæ‰å¯ä»¥è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œè€Œä¸æ˜¯åƒå€¼æ–¹æ³•é‚£æ ·ï¼Œæ¯ä¸€ä¸ª\(s,a,r,s'\)éƒ½å¯ä»¥è¿›è¡Œå‚æ•°æ›´æ–°ã€‚å¦‚æœåœ¨probå¾ˆå°çš„æƒ…å†µä¸‹, å¾—åˆ°äº†ä¸€ä¸ªå¤§çš„Reward, ä¹Ÿå°±æ˜¯å¤§çš„vt, é‚£ä¹ˆ-log\(prob\)\*vtå°±æ›´å¤§, è¡¨ç¤ºæ›´åƒæƒŠ, \(æˆ‘é€‰äº†ä¸€ä¸ªä¸å¸¸é€‰çš„åŠ¨ä½œ, å´å‘ç°åŸæ¥å®ƒèƒ½å¾—åˆ°äº†ä¸€ä¸ªå¥½çš„ reward, é‚£æˆ‘å°±å¾—å¯¹æˆ‘è¿™æ¬¡çš„å‚æ•°è¿›è¡Œä¸€ä¸ªå¤§å¹…ä¿®æ”¹\)ã€‚

è¿™å°±æ˜¯ -log\(prob\)\*vtçš„ç‰©ç†æ„ä¹‰å•¦.Policy Gradientçš„æ ¸å¿ƒæ€æƒ³æ˜¯æ›´æ–°å‚æ•°æ—¶æœ‰ä¸¤ä¸ªè€ƒè™‘ï¼šå¦‚æœè¿™ä¸ªå›åˆé€‰æ‹©æŸä¸€åŠ¨ä½œï¼Œä¸‹ä¸€å›åˆé€‰æ‹©è¯¥åŠ¨ä½œçš„æ¦‚ç‡å¤§ä¸€äº›ï¼Œç„¶åå†çœ‹å¥–æƒ©å€¼ï¼Œå¦‚æœå¥–æƒ©æ˜¯æ­£çš„ï¼Œé‚£ä¹ˆä¼šæ”¾å¤§è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¦‚æœå¥–æƒ©æ˜¯è´Ÿçš„ï¼Œå°±ä¼šå‡å°è¯¥åŠ¨ä½œçš„æ¦‚ç‡ã€‚

1. ç®—æ³•è¾“å‡ºçš„æ˜¯åŠ¨ä½œçš„æ¦‚ç‡ï¼Œè€Œä¸æ˜¯Qå€¼ã€‚
2. æŸå¤±å‡½æ•°çš„å½¢å¼ä¸ºï¼šloss= -log\(prob\)\*vt
3. éœ€è¦ä¸€æ¬¡å®Œæ•´çš„episodeæ‰å¯ä»¥è¿›è¡Œå‚æ•°çš„æ›´æ–°

### â¡ Log likelihood or likelihood

> [https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution](https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution)

1. It is extremely useful for example when you want to calculate the _joint likelihood_ for a set of independent and identically distributed points. Assuming that you have your points:X={x1,x2,â€¦,xN}X={x1,x2,â€¦,xN}The total likelihood is the product of the likelihood for each point, i.e.:p\(Xâˆ£Î˜\)=âˆi=1Np\(xiâˆ£Î˜\)p\(Xâˆ£Î˜\)=âˆi=1Np\(xiâˆ£Î˜\)where Î˜Î˜ are the model parameters: vector of means Î¼Î¼ and covariance matrix Î£Î£. If you use the log-likelihood you will end up with sum instead of product:lnp\(Xâˆ£Î˜\)=âˆ‘i=1Nlnp\(xiâˆ£Î˜\)lnâ¡p\(Xâˆ£Î˜\)=âˆ‘i=1Nlnâ¡p\(xiâˆ£Î˜\)
2. Also in the case of Gaussian, it allows you to avoid computation of the exponential:

   p\(xâˆ£Î˜\)=1\(2Ï€â€¾â€¾â€¾âˆš\)ddetÎ£â€¾â€¾â€¾â€¾â€¾âˆšeâˆ’12\(xâˆ’Î¼\)TÎ£âˆ’1\(xâˆ’Î¼\)p\(xâˆ£Î˜\)=1\(2Ï€\)ddetÎ£eâˆ’12\(xâˆ’Î¼\)TÎ£âˆ’1\(xâˆ’Î¼\)Which becomes:

   lnp\(xâˆ£Î˜\)=âˆ’d2ln\(2Ï€\)âˆ’12ln\(detÎ£\)âˆ’12\(xâˆ’Î¼\)TÎ£âˆ’1\(xâˆ’Î¼\)lnâ¡p\(xâˆ£Î˜\)=âˆ’d2lnâ¡\(2Ï€\)âˆ’12lnâ¡\(detÎ£\)âˆ’12\(xâˆ’Î¼\)TÎ£âˆ’1\(xâˆ’Î¼\)

3. Like you mentioned lnxlnâ¡x is a monotonically increasing function, thus log-likelihoods have the same relations of order as the likelihoods:

   p\(xâˆ£Î˜1\)&gt;p\(xâˆ£Î˜2\)â‡”lnp\(xâˆ£Î˜1\)&gt;lnp\(xâˆ£Î˜2\)p\(xâˆ£Î˜1\)&gt;p\(xâˆ£Î˜2\)â‡”lnâ¡p\(xâˆ£Î˜1\)&gt;lnâ¡p\(xâˆ£Î˜2\)

4. From a standpoint of computational complexity, you can imagine that first of all summing is less expensive than multiplication \(although nowadays these are almost equal\). But what is even more important, likelihoods would become very small and you will run out of your floating point precision very quickly, yielding an underflow. That's why it is way more convenient to use the logarithm of the likelihood. Simply try to calculate the likelihood by hand, using pocket calculator - almost impossible.

   Additionally in the classification framework you can simplify calculations even further. The relations of order will remain valid if you drop the division by 22 and the dln\(2Ï€\)dlnâ¡\(2Ï€\) term. You can do that because these are class independent. Also, as one might notice if variance of both classes is the same \(Î£1=Î£2Î£1=Î£2\), then you can also remove the ln\(detÎ£\)lnâ¡\(detÎ£\) term.


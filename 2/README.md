---
description: Paper summary
---

# Paper NoteğŸ“

## The Plan of reading paper ğŸŒ€

1. continuous control

> ä¸‹é¢æ˜¯æš‚å­˜äºæ­¤æœªåˆ†ç±»æ–‡ç« è®°å½•

## Learning to act by predicting the future.

> ICLR 2017, intel labs.  
> sensorimotor æ„Ÿè§‰è¿åŠ¨çš„; immersive èº«ä¸´å…¶å¢ƒçš„; extraneous å¤–éƒ¨çš„,æ— å…³çš„; extensive experiment ,å¹¿æ³›çš„,å¤§é‡çš„; sophisticated å°–ç«¯çš„; assist å¸®åŠ©; overcome challenges;  depart from... ä¸åŒäº; constitute ç»„æˆ,æ„æˆ; pertain to é€‚åˆ,å±äº,å…³äº; ammunition å¼¹è¯; in terms of... åœ¨..æ–¹é¢; occasional å¶å°”çš„; take ... into account, æŠŠ...è€ƒè™‘è¿›å»; perspective,å‰æ™¯,é€è§†,è§‚ç‚¹,æƒ³æ³•; date back decades; advantageous, æœ‰åˆ©çš„;

1. æœ¬æ–‡æ–¹æ³•ä½¿ç”¨é«˜ç»´ä¼ æ„Ÿå™¨æµå’Œä½ç»´åº¦æµ‹é‡æµ;è¿™ä¸ªç»“æ„æä¾›ç›‘ç£ä¿¡å·å¯ä»¥åœ¨äº¤äº’ä¸­è®­ç»ƒè¿åŠ¨æ„ŸçŸ¥æ§åˆ¶æ¨¡å‹.è¿™ä¸ªæ¨¡å‹ä½¿ç”¨ç›‘ç£å­¦ä¹ æŠ€æœ¯è®­ç»ƒä¸éœ€è¦ extraneous supervision. å›¾åƒè¾“å…¥. supervised learning is concerned with learning input-output mappings, unsupervised learning aims to find hidden structure in data, and reinforcement learning deals with goal-directed behaviour.
2. æœ¬æ–‡æ–¹æ³•ä¸åŒäºå¥–åŠ±æœºåˆ¶çš„ RL,è€Œæ˜¯ä½¿ç”¨ sensory input stream $$\{s_t\}$$, å’Œ measurements $$\{m_t\}$$; sensory stream é€šå¸¸æ˜¯é«˜ç»´çš„åŒ…å«å›¾åƒ,å£°éŸ³ç­‰ç»†èŠ‚,measurement stream ä½ç»´çš„é€šå¸¸æ˜¯æ­¤åˆ» state; åè€…å¤§å¤šæ•°ä¸ºå§¿æ€,ç»“æ„,è¡€è“å€¼,å…³å¡ç­‰ç­‰ç›¸å…³çš„çŠ¶æ€é‡; æœ¬æ–‡ guiding observation æŠŠ temporal structure of sensory å’Œ measurement streams interlock äº’é”èµ·æ¥,å½¢æˆäº†ä¸°å¯Œçš„ç›‘ç£çš„ä¿¡å·; ç»™äº† sensory input, measurements, å’Œ goal, æ™ºä½“å°±å¯ä»¥è®­ç»ƒæˆå¯ä»¥é¢„æµ‹åœ¨æœªæ¥ measurements ä¸‹ä¸åŒactionsçš„ç»“æœäº†. ä¸¤ä¸ªé‡è¦ä¼˜ç‚¹: 1-ä¸å¶å°”çš„æ ‡é‡ reward ç›¸æ¯”,measurements stream æä¾›ä¸°å¯Œçš„å’Œç¨ å¯†çš„ç›‘ç£å¯ä»¥ç¨³å®šå¹¶åŠ é€Ÿè®­ç»ƒ. 2-æå‡ºçš„å…¬å¼è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦æœ‰æ˜ç¡®çš„ç›®æ ‡,ä»–èƒ½é¢„æµ‹å°†æ¥çš„ç›®æ ‡; 

## Lists of Paper ğŸŒ€

#### DRL paper list

{% embed url="http://www.voidcn.com/article/p-rlbfnjbt-gc.html " %}

è¿™ä¸ªç½‘é¡µåˆ—å‡ºå¢å¼ºå­¦ä¹ æ–‡ç« ä¸åˆ†ç±»åˆ—è¡¨,å¾ˆæœ‰å‚è€ƒä»·å€¼,åˆ†ç±»è¯¦ç»†.

æ”¹è¿›ç›®æ ‡Qå€¼è®¡ç®—ï¼šDeep Reinforcement Learning with Double Q-learning 

æ”¹è¿›éšæœºé‡‡æ ·ï¼šPrioritized Experience Replay 

æ”¹è¿›ç½‘ç»œç»“æ„ï¼Œè¯„ä¼°å•ç‹¬åŠ¨ä½œä»·å€¼ï¼šDueling Network Architectures for Deep Reinforcement Learning \( æœ¬æ–‡ä¸ºICMLæœ€ä½³è®ºæ–‡ä¹‹ä¸€ï¼‰ 

æ”¹è¿›æ¢ç´¢çŠ¶æ€ç©ºé—´æ–¹å¼ï¼šï¼ˆ1ï¼‰Deep Exploration via Bootstrapped DQN ï¼ˆ2ï¼‰Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models 

æ”¹å˜ç½‘ç»œç»“æ„ï¼Œå¢åŠ RNNï¼šDeep Recurrent Q-Learning for Partially Observable MDPsï¼ˆéDeepMindå‡ºå“ï¼Œæ•ˆæœå¾ˆä¸€èˆ¬ï¼Œè°ˆä¸ä¸Šæ”¹è¿›ï¼Œæœ¬æ–‡ä¹Ÿä¸è€ƒè™‘è®²è§£ï¼‰ 

å®ç°DQNè®­ç»ƒçš„è¿ç§»å­¦ä¹ ï¼šï¼ˆ1ï¼‰Policy Distillation ï¼ˆ2ï¼‰ Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning 

è§£å†³é«˜éš¾åº¦æ¸¸æˆMontezumaâ€˜s Revengeï¼šUnifying Count-Based Exploration and Intrinsic Motivation 

åŠ å¿«DQNè®­ç»ƒé€Ÿåº¦ï¼šAsynchronous Methods for Deep Reinforcement Learning ï¼ˆè¿™ç¯‡æ–‡ç« è¿˜å¼•å‡ºäº†å¯ä»¥æ›¿ä»£DQNçš„A3Cç®—æ³•ï¼Œæ•ˆæœ4å€Nature DQNï¼‰ 

æ”¹å˜DQNä½¿ä¹‹èƒ½å¤Ÿåº”ç”¨åœ¨è¿ç»­æ§åˆ¶ä¸Šé¢ï¼šContinuous Deep Q-Learning with Model-based Acceleration

{% embed url="https://github.com/junhyukoh/deep-reinforcement-learning-papers/blob/master/README.md\#continuous-control" caption="all DRL" %}

 

#### 

#### GAN paperlist

{% embed url="https://github.com/zhangqianhui/AdversarialNetsPapers   " %}

{% embed url="https://github.com/hindupuravinash/the-gan-zoo" %}

#### 

#### Multi-agent

{% embed url="http://lantaoyu.com/posts/2017/06/marl-papers/" %}


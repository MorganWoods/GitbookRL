---
description: 'continuous control methods,'
---

# Continuous control

## Actor-critic seriesğŸŒ€

* Policy network select actions as an actor, value-based critic estimate the behavior of policy. update policy parameters according to critic's output.
* The proof that AC methods converge was shown in a brilliant Masters thesis by Konda

### Actor-Critic algorithms

> Konda, mit.

These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic.

### Asynchronous Methods for Deep Reinforcement Learning \(A3C\)

> Volodymyr Mnih. DeepMind. 2016.

* å¼‚æ­¥æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œ,æå‡ºäº†è®¸å¤šå¼‚æ­¥å˜ä½“. è¡¨ç°æœ€å¥½çš„å˜ä½“æ˜¯åº”ç”¨åœ¨ actor critic ä¸Šçš„å¹¶ä¸”è¶…è¿‡ Atari ä¸Šçš„æœ€æ–°æ°´å¹³.ä½¿ç”¨å¤šæ ¸ CPU è¿è¡Œç¨‹åº.è§£å†³ AC ç®—æ³•ä¸æ”¶æ•›é—®é¢˜.åŒæ—¶åˆ›å»ºå¤šä¸ªå¹¶è¡Œç¯å¢ƒ,å¤šä¸ª agent éš”ç¦»è¿è¡Œ.ä¸»ç»“æ„çš„å‚æ•°æ›´æ–°å—åˆ°å‰¯ç»“æ„æäº¤æ›´æ–°çš„ä¸è¿ç»­æ€§å¹²æ‰°.æ›´æ–°ç›¸å…³åº¦é™ä½,æ”¶æ•›æ€§æé«˜.
* DRL åŸºäº experience replay, ä½†ä»–çš„ç¼ºç‚¹æ˜¯:æ¯æ¬¡äº¤äº’éœ€è¦è®¡ç®—,éœ€è¦ç¦»ç­–ç•¥å­¦ä¹ ç®—æ³•.æœ¬æ–‡ä½¿ç”¨**å¤šçº¿ç¨‹å–ä»£äº† experience replay.**æœ¬æ–‡æå‡ºçš„ç®—æ³•: asynchronous one-step Q-learning; Asynchronous advantage actor-critic;

### Benchmarking Deep Reinforcement Learning for Continuous Control

> Yan Duan, OpenAI . 2016

* ä¸€ä»½ Benchmark. æ–‡ä¸­åˆ—ä¸¾çš„ç®—æ³•æœ‰: Reinforce;**TNPG**;RWR;REPS;**TRPO**;CEM;CMA-ES; **DDPG**
* å…¶ä¸­, TRPO ä¸ TNPG æ•ˆæœä¼˜ç§€. æœ¬æ–‡ä»‹ç»äº†å‚æ•°çš„é€‰å–ä¸å¾®è°ƒ.
* å¯¹ DDPG çš„è¯„ä»·: æŸäº›ä»»åŠ¡æ”¶æ•›å¿«,ç”±äºæ ·æœ¬çš„é«˜æ•ˆç‡.ä½†æ˜¯ä¸ç¨³å®š.å¹¶ä¸”è®­ç»ƒè¿‡ç¨‹ä¸­ policy è¡¨ç°é™çº§.å¯¹ reward çš„èŒƒå›´è¾ƒæ•æ„Ÿ,æœ¬æ–‡å®éªŒæŠŠ reward è®¾ç½®ä¸º0.1,ä¼¼ä¹æé«˜äº†ç¨³å®šæ€§.
* TNPG å’Œ TRPO: æ­¤äºŒè€…è¡¨ç°å°¤å…¶çªå‡º,è¯æ˜äº†é™åˆ¶ policy åˆ†å¸ƒçš„æ”¹å˜å¯ä»¥æé«˜å­¦ä¹ çš„ç¨³å®šæ€§.
* æ–‡æœ«è¡¥å……ææ–™ä¸­æœ‰å®éªŒç»†èŠ‚,å¯ä»¥ä½œä¸ºå‚è€ƒ.

### Combining Deep Q-Learning and Advantage Actor-Critic for Continuous Control \(A2S\)

> Rae C.Jeong, Waterloo,2018
>
>  æºç  [https://github.com/raejeong/RaeboSchool](https://github.com/raejeong/RaeboSchool)

* æœ¬æ–‡ä»£ç ä¸ DDPG çš„å”¯ä¸€åŒºåˆ«æ˜¯åœ¨é€‰æ‹©æœ€ä¼˜åŠ¨ä½œæ–¹é¢: DDPG æ˜¯ PG ç½‘ç»œ\( actor\)ç›´æ¥é€‰æ‹©åŠ¨ä½œå¹¶æ‰§è¡Œ, DQN ç½‘ç»œ\(critic\)è¿›è¡Œä»·å€¼è¯„ä¼°; æ­¤æ–‡æ˜¯æ ¹æ® policy ç½‘ç»œåˆ†å¸ƒé€‰æ‹©å‡ ä¸ªå€™é€‰åŠ¨ä½œ,å†ç”¨ DQN ç½‘ç»œä»·å€¼è¯„ä¼°,å­˜å‚¨ä»·å€¼æœ€é«˜åŠ¨ä½œ. å¹¶æ‰§è¡Œ.\(æŠŠç¡®å®šæ€§å¤šåšç­–ç•¥æ”¹ä¸ºä¸ç¡®å®šæ€§åŠ¨ä½œç­–ç•¥\)
* å®éªŒæ•ˆæœä¼˜äº A2C

### Combining Policy Gradient And Q-Learning \(PGQL\)

> Brendan O'Donoghue,ICLR,2017

* ä¸ DDPG é½åçš„ PGQL; ç»“åˆ PG ä¸ç¦»ç­–ç•¥ Q-Learning,
* Q æ›´æ–°æ–¹ç¨‹åšäº†æ”¹å˜.

### Continuous Control with Deep Reinforcement Learning \(DDPG\) 

> 2016å¹´ICLR, Author: Timothy P.Lillicrap

* DDPG ç®—æ³•ä»è¿™é‡Œæå‡º. AC ç®—æ³•åŠ  DQN ç®—æ³•. æ˜¯ AC çš„å‡çº§ç‰ˆ.

### Continuous Deep Q-Learning with Model-based Acceleration \(NAF\)

> 2016å¹´, Cambridge, ä½œè€…: Shixiang Gu

* é«˜ç»´RL åŸºäº model-free ä¼šå¢åŠ é‡‡æ ·çš„å¤æ‚ç¨‹åº¦,è¿™ç¯‡æ–‡ç« ä¸»è¦é™ä½è¿ç»­ç©ºé—´é‡‡æ ·å¤æ‚åº¦.æå‡º NAF å¯ä»¥ä½¿ç”¨ Qlearning å¤„ç†è¿ç»­ä»»åŠ¡.ä½¿ç”¨ model-based åŠ é€Ÿ; model-based: ä½¿ç”¨ç›‘ç£å­¦ä¹ å¹¶ä¸”åœ¨ model ä¸‹ä¼˜åŒ–å‡ºçš„ç­–ç•¥.
* æœ¬æ–‡ä¸‰ä¸ªè´¡çŒ®: Qlearning åœ¨è¿ç»­ç©ºé—´çš„ä½¿ç”¨; learned model; åŠ é€Ÿ model free è¿ç»­å­¦ä¹ .
* NAF: ä¸éœ€è¦åƒ DDPG é‚£æ ·è®­ç»ƒä¸¤ä¸ªç½‘ç»œ,åªéœ€è¦è®­ç»ƒä¸€ä¸ª;æå‡ºç®—æ³•åå­—:continuous Q learning with NAF.
* ä»æ–‡ç« å®éªŒæ¥çœ‹, NAF çš„æ›´ç¨³å®š,å¹¶ä¸” reward é«˜,ä¸ DDPG ç›¸æ¯”.
* ä¸ºä»€ä¹ˆè¿™ä¸ª NAF çš„ Qlearning å¯ä»¥åœ¨è¿ç»­ç©ºé—´è¿ç”¨? â“ æ€ä¹ˆé€‰æ‹©åŠ¨ä½œçš„? é€‰æ‹©åŠ¨ä½œçš„ç½‘ç»œå¦‚ä½•æ›´æ–°? å’Œ DQN çš„æ›´æ–°æ–¹å¼æœ‰åŒºåˆ«ä¹ˆ?

### Deterministic Policy Gradient Algorithms \(DPG\)    

> 2014, David Silver ,DeepMind

* DDPG base on DPG, which is deterministic actor-critic method.
* This paper prove policy gradient. adjust parameters to maximum object $$J(\pi_\theta))$$ function.
* Equivalence Between Policy Gradients and Soft Q-Learning
* The deterministic policy gradient is relative to stochastic PG.
* However, we show that the deterministic policy gradient does indeed exist, and further-more it has a simple model-free form that simply follows the gradient of the action-value function.
* In the stochastic case, the policy gradient integrates over both state and action spaces, whereas in the deterministic case it only integrates over the state space.
* To ensure that our deterministic policy gradient algorithms continue to explore satisfactorily, we introduce an off-policy learning algorithm. The basic idea is to choose actions according to a stochastic behaviour policy \(to ensure adequate exploration\), but to learn about a deterministic target policy \(exploiting the efficiency of the deterministic policy gradient\).

### High-Dimensional Continuous Control Using Generalized Advantage Estimation \(GAE\)

> John Schulman, ICLR, 2016

* Policy Gradient æ–¹æ³•å¯¹äº RLçš„ä¸¤ä¸ªæŒ‘æˆ˜:éœ€è¦å¤§é‡çš„æ ·æœ¬æ•°æ®;å°½ç®¡è¾“å…¥æ•°æ®ä¸ç¨³å®š,ä½†æ˜¯ä¹Ÿå¾ˆéš¾å¾—åˆ°ç¨³å®šçš„å’ŒæŒç»­çš„æé«˜. è§£å†³å‰è€…ä½¿ç”¨ value function å‡å°‘æ–¹å·®; è§£å†³åè€…ä½¿ç”¨ **trust region optimization**.
* ä½¿ç”¨äºŒè¶³å››è¶³æœºå™¨äººåšå®éªŒ, model free çš„.
* æœ¬æ–‡è´¡çŒ®: æå‡º GAE æ¥çº æ­£å’Œæé«˜ PG çš„ variance reduction çš„æœ‰æ•ˆæ€§;æå‡ºå¯¹äº Value function ä½¿ç”¨ TRO æ–¹æ³•,å¥å£®æœ‰æ•ˆæ¥è®­ç»ƒç¥ç»ç½‘ç»œ;ç»“åˆä¸ŠäºŒè€…,è·å¾—äº†åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æœ‰æ•ˆçš„ç­–ç•¥.
* ä¸€ä¸ªå¯èƒ½çš„æœªæ¥å·¥ä½œæ˜¯å¦‚ä½•è‡ªé€‚åº”æˆ–è‡ªåŠ¨è°ƒèŠ‚ä¼°è®¡å‚æ•° 
* > æ‘˜è‡ª [å¤©æ´¥åŒ…å­é¦…](https://zhuanlan.zhihu.com/p/29486661) : åŸºçº¿å‡½æ•°çš„å¼•ç”¨å¯ä»¥å‡å° PG çš„æ–¹å·®. A\( ä¼˜åŠ¿å‡½æ•°\)æ˜¯åŠ¨ä½œå€¼å‡½æ•°ç›¸å¯¹äºå€¼å‡½æ•°çš„ä¼˜åŠ¿, è‹¥åŠ¨ä½œå€¼å‡½æ•°æ¯”å€¼å‡½æ•°å¤§,é‚£ä¹ˆåˆæ˜¯å‡½æ•°ä¸ºæ­£,åä¹‹äº¦å; å¯¹äº PG,ä¼˜åŠ¿å‡½æ•°ä¸ºæ­£æ—¶ï¼Œå…¶å¹…å€¼ä¸ºæ­£ï¼Œåˆ™å‚æ•°æ²¿ç€ä½¿å¾—è¯¥è½¨è¿¹æ¦‚ç‡å¢å¤§çš„æ–¹å‘æ›´æ–°ï¼›ä¼˜åŠ¿å‡½æ•°ä¸ºè´Ÿæ—¶ï¼Œç­–ç•¥æ¢¯åº¦çš„å¹…å€¼ä¸ºè´Ÿï¼Œåˆ™å‚æ•°æ²¿ç€ä½¿å¾—è¯¥è½¨è¿¹å‡å°çš„æ–¹å‘æ›´æ–°ã€‚å› æ­¤ï¼Œé‡‡ç”¨ä¼˜åŠ¿å‡½æ•°æ—¶ï¼Œç®—æ³•çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚
  >
  > ç„¶è€Œï¼Œæ ¹æ®ä¼˜åŠ¿å‡½æ•°å®šä¹‰ ![](https://www.zhihu.com/equation?tex=A%5E%7B%5Cpi%7D%5Cleft%28s_t%2Ca_t%5Cright%29%3DQ%5Cleft%28s_t%2Ca_t%5Cright%29-V%5Cleft%28s_t%5Cright%29)ï¼Œä¼˜åŠ¿å‡½æ•°ä¸­çš„å€¼å‡½æ•°å¸¸å¸¸åˆ©ç”¨é€¼è¿‘ç®—æ³•è¿‘ä¼¼è®¡ç®—ï¼Œå› æ­¤å¾€å¾€ä¼šå¼•å…¥åå·®ã€‚
  >
  > ä¼˜åŠ¿å‡½æ•°çš„ä¸€æ­¥ä¼°è®¡å¯å†™ä¸ºï¼š
  >
  > ä»ä¼˜åŠ¿å‡½æ•°çš„ä¸€æ­¥ä¼°è®¡ä¸­æˆ‘ä»¬çœ‹åˆ°ï¼Œ ![](https://www.zhihu.com/equation?tex=V%5Cleft%28s_t%5Cright%29)å’Œ ![](https://www.zhihu.com/equation?tex=V%5Cleft%28s_%7Bt%2B1%7D%5Cright%29)çš„çœŸå®å€¼éƒ½æ˜¯æœªçŸ¥çš„ï¼Œè€Œæ˜¯ç”¨åˆ°äº†ä¼°è®¡å€¼ï¼Œå› æ­¤ä¼˜åŠ¿å‡½æ•°å­˜åœ¨åå·®ã€‚
  >
  > GAEçš„æ–¹æ³•æ˜¯æ”¹è¿›å¯¹ä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡ï¼Œå°†åå·®æ§åˆ¶åˆ°ä¸€å®šçš„èŒƒå›´å†…ã€‚å…¶æ–¹æ³•æ˜¯å¯¹ä¼˜åŠ¿å‡½æ•°è¿›è¡Œå¤šæ­¥ä¼°è®¡ï¼Œå¹¶å°†è¿™äº›å¤šæ­¥ä¼°è®¡åˆ©ç”¨è¡°å‡å› å­è¿›è¡Œç»„åˆ
  >
  > Shulman æå‡ºå¹¿ä¹‰ä¼˜åŠ¿å‡½æ•°ä¼°è®¡ ![](https://www.zhihu.com/equation?tex=GAE%5Cleft%28%5Cgamma+%2C%5Clambda%5Cright%29)ï¼Œåˆ©ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ä»1æ­¥åˆ°æ— ç©·æ­¥çš„ä¼˜åŠ¿å‡½æ•°ä¼°è®¡
  >
  > è¯¥æ–¹æ³•å¾ˆå¥½çš„è¯„ä»·äº†åå·®å’Œæ–¹å·®. åˆ©ç”¨ä»£æ›¿ACæ–¹æ³•ä¸­çš„Criticä¼šäº§ç”Ÿæ›´å¥½çš„æ•ˆæœ? ? ?

### Learning Continuous Control Policies by Stochastic Value Gradients \(SVG\)

> Nicolas Heess, DeepMind ,2015

* æå‡ºæ¡†æ¶ä½¿ç”¨åå‘ä¼ æ’­å­¦ä¹ è¿ç»­åŠ¨ä½œç©ºé—´ç­–ç•¥. é€šè¿‡åœ¨ bellman ç­‰å¼çš„ç¡®å®šæ€§å‡½æ•°ä¸­å¢åŠ å™ªå£°æ¥å¢åŠ ç­–ç•¥éšæœºæ€§.
* æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯ SVG, é€šè¿‡ re-parameterization æŠŠå™ªå£°å¼•å…¥åˆ°ç­–ç•¥å’Œ model ä¸­.

### Policy Gradient Methods for Reinforcement Learning with Function Approximation \(PG\)

> Richard S.Sutton, 1999

* è¯æ˜äº† PG å¯ä»¥æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ç‚¹. ä¼˜åŒ–å‡½æ•°æ˜¯ RL å¿…è¦çš„ç¯èŠ‚, ä½†æ˜¯æ ‡å‡†çš„æ–¹æ³•ä¼˜åŒ–ä»·å€¼å‡½æ•°å’Œå†³å®šç­–ç•¥ä»ç†è®ºä¸Šéš¾ä»¥è¯æ˜.è¿™ç¯‡æ–‡ç« ä¸­æˆ‘ä»¬æ¢ç´¢äº†ä¸€ä¸ªå¯ä¾›æ›¿ä»£çš„æ–¹æ³•,ç­–ç•¥å¯ä»¥æ˜ç¡®åœ°ç”¨ä»–è‡ªå·±çš„å‡½æ•°è¿‘ä¼¼è€…è¡¨æ˜, ç‹¬ç«‹äºä»·å€¼å‡½æ•°,å¹¶ä¸”æ ¹æ®å…³äºç­–ç•¥å‚æ•°çš„æ¢¯åº¦æœŸæœ›æ¥æ›´æ–°.æˆ‘ä»¬ä¸»è¦çš„ç»“æœè¡¨æ˜æ¢¯åº¦å¯ä»¥å†™æˆé€‚åº”äºè¿‘ä¼¼åŠ¨ä½œå€¼å’Œåˆ©ç›Šæ–¹ç¨‹çš„å½¢å¼.ä½¿ç”¨è¿™ä¸ªç»“æœ,æˆ‘ä»¬é¦–æ¬¡è¯æ˜ä½¿ç”¨ä»»æ„çš„å¾®åˆ†è¿‘ä¼¼æ–¹ç¨‹çš„ç­–ç•¥è¿­ä»£å¯ä»¥æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ç­–ç•¥.
* the value-function approach has worked well in many applications, but has several limitations. first, it is oriented toward finding deterministic policies, whereas the optimal policy is often stochastic, selecting different actions with specific probabilities. Second, an arbitrarily small change in the estimated value of an action can cause it to be, or not be, selected.

### Q-prop: Sample-Efficent Policy Gradient With An Off-Policy Critic \(Q-Prop\)

> Shixiang Gu, Cambridge, ICLR, 2017

* åœ¨é¢å‘ç°å®å®éªŒä¸­, model free DRL ä¼šé¢å¯¹ä¸€ä¸ªé«˜é‡‡æ ·å¤æ‚åº¦. Batch PG æ–¹æ³•å¯ä»¥ç¨³å®šå­¦ä¹ ,ä½†æ˜¯ cost é«˜æ–¹å·®,éœ€è¦å¤§é‡çš„ batches; TD æ–¹æ³•å¦‚ AC å’Œ QL, é‡‡æ ·å¤šä½†æ˜¯æœ‰åå·®. è¿™ç¯‡æ–‡ç« æå‡ºæ–¹æ³•: **ç»“åˆ PG çš„ç¨³å®šæ€§ä¸ off policy RL çš„æ•ˆç‡**.æå‡ºäº† Q prop æ–¹æ³•,ä»–æ—¢æœ‰é‡‡æ ·æ•ˆç‡ä¹Ÿæœ‰ç¨³å®šæ€§. é™ä½æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®å¹¶ä¸”ä¸å¼•å…¥åç½®.
* æ ¸å¿ƒæƒ³æ³•: ä½¿ç”¨ critic çš„ first-order Taylor expansion \(ä¸€é˜¶æ³°å‹’å±•å¼€\) ä½œä¸ºæ§åˆ¶å˜é‡.è§£ææ¢¯åº¦é¡¹è¯„ä»·è¿‡ç¨‹,å®ƒæ—¢å¯ä»¥è¢«çœ‹åšä½¿ç”¨off-policyçš„è¯„ä»·è¿‡ç¨‹æ¥å‡å°ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¸¦æ¥çš„æ–¹å·®ï¼Œåˆè¢«çœ‹ä½œä½¿ç”¨on-policyè’™ç‰¹å¡æ´›æ–¹æ³•æ¥ä¿®æ­£è¯„ä»·æ¢¯åº¦æ–¹æ³•å¸¦æ¥çš„åå·®ã€‚
* PG é™¤äº†é«˜æ–¹å·®,å¦ä¸€é—®é¢˜åœ¨äºä»–éœ€è¦ on-policy æ ·æœ¬,è¿™ä½¿å¾—ä»–å¯¹æ ·æœ¬æ•æ„Ÿ. Q prop ç”¨æ¥ä¼°è®¡ PG çš„.
* é€šè¿‡ä½¿ç”¨ç¡®å®šæ€§æœ‰åä¼°è®¡ä½œä¸ºæ§åˆ¶å˜é‡çš„ç‹¬ç‰¹å½¢å¼å¯¹äº MC PG ä¼°è®¡å™¨,æˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨ä¸¤ç§å½¢å¼çš„æ¢¯åº¦ä¿¡æ¯æ¥å»ºé€ ä¸€ä¸ªæ–°ä¼°è®¡å™¨åœ¨å®è·µä¸­æé«˜æ ·æœ¬æ•ˆç‡é€šè¿‡åŒ…å« off policy é‡‡æ ·åŒæ—¶ä¿ç•™ on policy MC PG çš„ç¨³å®šæ€§.

### Trust Region Policy Optimization \(TRPO\)

> John Schulman, University of California, 2015

![](https://pic3.zhimg.com/80/v2-e519a12e0617dd0eb66de29db96af429_hd.jpg)

* åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­è¿˜æœ‰å¾ˆå¤šæœ‰æ„æ€çš„è¯¾é¢˜ï¼Œæ¯”å¦‚ç›¸å®¹å‡½æ•°æ³•ï¼Œè‡ªç„¶æ¢¯åº¦æ³•ç­‰ç­‰ã€‚ä½†Shulmanåœ¨åšå£«è®ºæ–‡ä¸­å·²è¯æ˜ï¼Œè¿™äº›æ–¹æ³•å…¶å®éƒ½æ˜¯TRPOå¼±åŒ–çš„ç‰¹ä¾‹ï¼Œè¯´è¿™äº›æ˜¯å†æ¬¡å¼ºè°ƒTRPOçš„å¼ºå¤§ä¹‹å¤„ã€‚ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„ç¡¬ä¼¤å°±åœ¨æ›´æ–°æ­¥é•¿ â€‹ï¼Œå½“æ­¥é•¿ä¸åˆé€‚æ—¶ï¼Œæ›´æ–°çš„å‚æ•°æ‰€å¯¹åº”çš„ç­–ç•¥æ˜¯ä¸€ä¸ªæ›´ä¸å¥½çš„ç­–ç•¥ï¼Œå½“åˆ©ç”¨è¿™ä¸ªæ›´ä¸å¥½çš„ç­–ç•¥è¿›è¡Œé‡‡æ ·å­¦ä¹ æ—¶ï¼Œå†æ¬¡æ›´æ–°çš„å‚æ•°ä¼šæ›´å·®ï¼Œå› æ­¤å¾ˆå®¹æ˜“å¯¼è‡´è¶Šå­¦è¶Šå·®ï¼Œæœ€åå´©æºƒã€‚æ‰€ä»¥ï¼Œåˆé€‚çš„æ­¥é•¿å¯¹äºå¼ºåŒ–å­¦ä¹ éå¸¸å…³é”®ã€‚æ‰€è°“åˆé€‚çš„æ­¥é•¿æ˜¯æŒ‡å½“ç­–ç•¥æ›´æ–°åï¼Œå›æŠ¥å‡½æ•°çš„å€¼ä¸èƒ½æ›´å·®ã€‚å¦‚ä½•é€‰æ‹©è¿™ä¸ªæ­¥é•¿ï¼Ÿæˆ–è€…è¯´ï¼Œå¦‚ä½•æ‰¾åˆ°æ–°çš„ç­–ç•¥ä½¿å¾—æ–°çš„å›æŠ¥å‡½æ•°çš„å€¼å•è°ƒå¢ï¼Œæˆ–å•è°ƒä¸å‡ã€‚è¿™æ˜¯TRPOè¦è§£å†³çš„é—®é¢˜ã€‚

  ![](../../.gitbook/assets/image%20%2810%29.png)

  å€¼å‡½æ•° ![](https://www.zhihu.com/equation?tex=V%28s%29)å¯ä»¥ç†è§£ä¸ºåœ¨è¯¥çŠ¶æ€ä¸‹æ‰€æœ‰å¯èƒ½åŠ¨ä½œæ‰€å¯¹åº”çš„åŠ¨ä½œå€¼å‡½æ•°ä¹˜ä»¥é‡‡å–è¯¥åŠ¨ä½œçš„æ¦‚ç‡çš„å’Œã€‚æ›´é€šä¿—çš„è®²ï¼Œå€¼å‡½æ•° ![](https://www.zhihu.com/equation?tex=V%28s%29)æ˜¯è¯¥çŠ¶æ€ä¸‹æ‰€æœ‰åŠ¨ä½œå€¼å‡½æ•°å…³äºåŠ¨ä½œæ¦‚ç‡çš„å¹³å‡å€¼ã€‚è€ŒåŠ¨ä½œå€¼å‡½æ•° ![](https://www.zhihu.com/equation?tex=Q%28s%2Ca%29)æ˜¯å•ä¸ªåŠ¨ä½œæ‰€å¯¹åº”çš„å€¼å‡½æ•°ï¼Œâ€‹$$Q-V$$â€‹èƒ½è¯„ä»·å½“å‰åŠ¨ä½œå€¼å‡½æ•°ç›¸å¯¹äºå¹³å‡å€¼çš„å¤§å°ã€‚æ‰€ä»¥ï¼Œè¿™é‡Œçš„ä¼˜åŠ¿æŒ‡çš„æ˜¯åŠ¨ä½œå€¼å‡½æ•°ç›¸æ¯”äºå½“å‰çŠ¶æ€çš„å€¼å‡½æ•°çš„ä¼˜åŠ¿ã€‚å¦‚æœä¼˜åŠ¿å‡½æ•°å¤§äºé›¶ï¼Œåˆ™è¯´æ˜è¯¥åŠ¨ä½œæ¯”å¹³å‡åŠ¨ä½œå¥½ï¼Œå¦‚æœä¼˜åŠ¿å‡½æ•°å°äºé›¶ï¼Œåˆ™è¯´æ˜å½“å‰åŠ¨ä½œè¿˜ä¸å¦‚å¹³å‡åŠ¨ä½œå¥½ã€‚

  â€‹

* ä¼˜åŒ–æ§åˆ¶ç­–ç•¥çš„æ–¹æ³•,å¯ä»¥ä¿è¯å•è°ƒé€’å¢.å¯¹æœ‰ä¼˜åŒ–å¤§é‡éçº¿æ€§ç­–ç•¥å¾ˆæœ‰æ•ˆ\(å¦‚ç¥ç»ç½‘ç»œ\).ä¸éœ€è¦å¤§é‡è°ƒè¯•è¶…å‚æ•°.
* è®¸å¤šç­–ç•¥ä¼˜åŒ–ç®—æ³•åˆ†ç±»ä¸ºä¸‰ä¸ª:policy iteration; policy gradient;derivative-free optimization å¦‚ cross entropy æ–¹æ³•\(CEM\) å’Œ covariance matrix adaptation\(CMA\), å®ƒæŠŠ cost è§†ä¸ºé»‘ç›’; å¯¹äºè¿ç»­æ§åˆ¶,å¦‚ CMA çš„æ–¹æ³•å¾ˆæˆåŠŸ; æœ¬æ–‡,ç¬¬ä¸€æ¬¡æå‡ºæœ€å°åŒ–ä¸€ä¸ªå›ºå®šçš„æ›¿ä»£ loss å‡½æ•°ä¿è¯ç­–ç•¥æé«˜é€šè¿‡ non-trivial step size, è¿˜æè¿°äº†ä¸¤ç§å˜ä½“,1 ,single-path æ–¹æ³•,è¿™ç§å¯ä»¥åº”ç”¨äº model-free ç¯å¢ƒ, 2 , vine æ–¹æ³•\(æ»•æ ‘\),éœ€è¦ç³»ç»Ÿåœ¨ç‰¹å®šçš„çŠ¶æ€ä¿å­˜,ä»…ä»…åœ¨ä»¿çœŸæ—¶å€™é€‚ç”¨.
* MDP$$(\mathcal{S} ,\mathcal{A} ,P,c,{\rho}_{0},\gamma)$$, å…¶ä¸­,$$c$$æ˜¯ cost function, $${\rho}_{0}$$æ˜¯åˆå§‹çŠ¶æ€çš„åˆ†å¸ƒ,$$\pi$$æ˜¯éšæœºç­–ç•¥,$${\eta}_{\pi}$$æ˜¯**æœŸæœ›è¡°å‡ cost**. ç„¶åæ˜¯ Q,V,A, ä¸å…¶ä»–çš„ä¸åŒ,è¿™é‡Œéƒ½æ˜¯ç”¨ cost ä»£æ›¿äº†å‰äººçš„ reward. $$\tilde{\pi}$$æ˜¯å¦ä¸€ä¸ªç­–ç•¥å…³æ³¨ çš„Advantage; æå‡ºæ˜¯éæ­£è§„åŒ–çš„è¡°å‡ visitation é¢‘ç‡:$$\rho_{\pi}(s)=(P(s_{0}=s)+\gamma P(s_{1}=s)+\gamma^{2} P(s_{2}=s)+...)$$.
* TRPO çš„åèº«æ˜¯ PPO, åˆ†å¸ƒå¼æ˜¯ DPPO.

### Reinforcement Learning with Deep Energy-Based Policies \(soft Q learning\)

> Tuomas Haarnoja, 2017

* â€‹

### Soft-Robust Actor-Critic Policy-Gradient \(SR-AC\)

> Esther Derman, Technion Israel institute, Google deepmind, 2018

* MDPs æœ‰äº›ä¸ç¡®å®šæ€§, æœ‰ä¸ª Robust MDPs è§£å†³è¿™ä¸ªé—®é¢˜. ä½†æ˜¯å®ƒå¸¦æ¥äº†è¿‡åˆ†è°¨æ…çš„ç»“æœ ; æœ¬æ–‡é›†ä¸­å­¦ä¹ ä¸€ä¸ª soft robust policy é€šè¿‡å¸æ”¶ soft robustness å’Œ online ac ç®—æ³• ; æœ¬æ–‡çš„è´¡çŒ®: 1, ä¸€ä¸ª soft-robust è¡ç”Ÿçš„ ç›®æ ‡å‡½æ•° ä¸ºäº† PG. 2. SRAC ç®—æ³•ä½¿ç”¨éšæœºä¼˜åŒ–æ¥å­¦ä¹  . 3. æ”¶æ•›è¯æ˜. 4. å®éªŒ,å±•ç¤ºäº†å…¶æ•ˆç‡.
* PG æ–¹æ³•ä¸€èˆ¬çš„ç›®æ ‡å‡½æ•°æ—¶æœ€å¤§åŒ–å¹³å‡ reward function.
* soft-robust ç›®æ ‡å‡½æ•°â€¦



## Naf

## Naf

## Naf

## Naf

## Naf

